\chapter{Outlook} \label{ch:outlook}

The research presented in this thesis explore ways in which data-driven approaches based on machine learning can be leveraged in the design-make-test cycle of drug discovery. In each of the three steps of `design', `make', and `test' we encounter the same underlying challenge, of grappling with the practical difficulties of a drug discovery campaign where we must make full use of the limited data available.

In Chapter \ref{ch:fresco} we looked at how to leverage fragment-protein structures from a crystallographic fragment screen to hit discovery in the absence of any bioactivity data. Using an unsupervised learning approach, we learn the geometric distribution of pharmacophores from the fragment-protein complexes, and use these to screen potential molecules for bioactivity. We showed that this approach outperforms docking on distinguishing active compounds from inactive ones on retrospective data. Further, we prospectively found novel hits for SARS-CoV-2 Mpro and the Mac1 domain of SARS-CoV-2 non-structural protein 3 by virtually screening a library of 1B molecules.

Chapter \ref{ch:ranking} takes us to the early stages of hit-to-lead molecular optimisation where bioactivity data is limited, noisy, and dominated by inactive molecules. We overcame this challenge with a learning-to-rank framework via an ML model that predicts whether a compound is more or less active than another. This approach allowed us to make use of inactive data and threshold the bioactivity differences above measurement noise, and validation on retrospective data for SARS-CoV-2 Mpro showed that we can outperform docking on ranking ligands. Combining this model with AI-based synthesis tools, we prospectively screened a library of 8.8M molecules to arrive at a potent compound with a novel scaffold.

While AI-based synthesis tools have already shown demonstrable success in accelerating the synthesis of new molecules, they are still prone to failure and suffer from a lack of transparency in their decision making due to their black-box nature. To address this, in Chapter \ref{ch:transformer} we showcased a workflow for quantitatively interpreting a state-of-the-art deep learning model for reaction prediction. By analysing chemically selective reactions, we showed examples of correct reasoning by the model, explain counterintuitive predictions, and identify Clever Hans predictions where the correct answer is reached for the wrong reason due to dataset bias.

In Chapter \ref{ch:testing} we explored how to accelerate testing procedures by applying machine learning on bioactivity data from nanomolar-scale high-thoughput chemistry. While this experimental technique greatly increases the number of molecules that can be tested, there is additional noise resulting from having to assay crude reaction mixtures instead of pure samples. Nevertheless, we showed that machine learning models trained on this data is able to cut through this noise and identify a false negative assay measurement, as well as prospectively screen a library of ~62K molecules to discover new SARS-CoV-2 Mpro inhibitors just as potent as those from the original assay.

\section{Directions for Future Research}

While we have explored some methods to accelerate the design-make-test cycle in this thesis, there is still much more work to be done to bring us closer to the dream of efficient and automated drug discovery. Below, we outline some of the most prominent directions for future research.

\subsection{Deploying and Extending ML-based Synthesis Tools}
Machine learning-based models for synthesis prediction have made significant strides in recent years, and are already sufficientily accurate to be deployed in industry for synthesis route design. However, translating these models into a practical tool that can be used by non-specialists is non-trivial. Connecting these models with commercial compound databases, updating models in response to changes in stock availability, and developing more user-friendly interfaces are fruitful engineering challenges for enabling more widespread adoption of these tools.

An intriguing extension of this idea is allowing a large language model (LLM) to access ML-based synthesis tools. Large language models, using the same architectures on which many ML synthesis tools are based on, have demonstrated incredible success in natural language processing as well as in understanding programming languages. An exciting use-case is to allow LLMs to make API calls to other software, meaning that one can abstract away complex programming tasks by just giving a natural language description of the task to the LLM. A recent example demonstrated how GPT-4, a powerful LLM, can perform molecular queries in the PubChem database, determine whether the molecule is purchasable, find a supplier that sells it and either purchase the compound or draft an email to a synthesis CRO to order. Integrating LLMs with ML-based synthesis tools, or augmenting LLMs by training them to understand SMILES and/or SMARTS, could extend such capabilities and facilitate complex user requests in synthesis planning.

In tandem with optimisng model deployment, continued improvement in the accuracy of synthesis predictions can be acheived in a number of ways. Novel model architectures that better reflect the underlying structure of chemical reaction data, both the graph nature of molecules as well as the correlations between chemical reactions from the same patent, could potentially improve the accuracy of synthesis predictions. The incorporation of proprietary reaction data from industry should also lead to improved performance, although there are difficulties in standardising data from electronic laboratory notebooks (ELNs) and there has been conflicting evidence regarding its usefulness. Finally, the creation of better benchmarks, likely incorporating disclosed industrial data, can help ensure that models are evaluated under realistic conditions, providing a more accurate and robust representation of their capabilities.

Last but not least, there is much room for improvment in the prediction of optimal reaction conditions and reaction yields, which are not currently handled by most ML-synthesis tools.  While existing models are effective at predicting the outcomes of small-scale chemical reactions, they may not be as accurate when it comes to predicting the behavior of reactions that must be scaled up for large-scale production. The inclusion of reaction yields and reaction conditions into ML synthesis models would allow for greater precision overall, and expand the scope of these models beyond small-scale reactions to bridge the gap between research and production.

\subsection{Making better use of existing data}

A common challenge in drug discovery is the limited availability of data. While it is true that data scarcity is a persistent issue, it is also true that we are not making the most of the data that we do have. Existing datasets can be enhanced by utilizing them in more effective ways, and by designing models specifically for low-data regimes. By adapting our approach, we can improve the accuracy of models and gain valuable insights into chemical space.

To overcome the challenge of data scarcity, we can adapt and design models specifically for low-data regimes. For example, imputation-based methods or further transfer learning can help us make more accurate predictions even with limited data. Imputation-based methods involve filling in missing data points with estimates derived from other data points, allowing researchers to make use of incomplete datasets. Transfer learning, on the other hand, involves using knowledge gained from one domain to inform the learning process in another domain. By developing and utilizing models that are specifically designed for low-data regimes, we can make more accurate predictions and gain insights that would not be possible with traditional models.

Federated learning is another promising approach that can help overcome data scarcity while maintaining data privacy. This method involves training models on data from multiple industry partners, allowing us to leverage a much larger dataset without compromising the privacy of the underlying data. One recent case study is the Molecular Entity Learning from Optimized Destruction and Deletions of Yeasts (MELODDY) project, which has brought together leading pharmaceutical companies to create a federated learning platform for drug discovery. By sharing data in a secure and private manner, we can make more accurate predictions and gain valuable insights into chemical space.

Finally, we can also leverage noisy but high-throughput experimental techniques to generate more data for drug discovery. One example of such a technique is DNA-encoded libraries, which enable the synthesis and screening of large numbers of compounds in a relatively short amount of time. While the resulting data may be noisy, the sheer volume of data generated makes it a valuable resource for model training and validation. By integrating data from these and other high-throughput techniques with traditional data sources, we can enhance our understanding of chemical space and improve the accuracy of drug discovery models.

\subsection{Integrate protein information into property prediction}
Accurate bioactivity prediction is still the primary bottleneck for computational drug design. Despite significant progress in recent years, predicting the efficacy of a drug candidate remains a complex challenge that involves a wide range of factors.

Recent advances in machine learning for protein bioinformatics offer promising new approaches to address this challenge. For example, researchers have developed deep learning models to predict protein structure and function, which can inform drug design strategies. By integrating these models and workflows with those for ML in chemistry and cheminformatics, we may be able to make significant breakthroughs in bioactivity prediction.

Expanding beyond bioactivity prediction, these models and workflows also have potential for predicting ADME/Tox properties, which can be even more challenging than bioactivity prediction. By improving our ability to predict how a drug candidate will interact with biological systems, we can design more effective and safer drugs.

Finally, these advances may also have applications in predicting the molecular mechanisms of action, as well as identifying new drug targets. By combining insights from both chemical and biological data, we can gain a more comprehensive understanding of how drugs interact with the body at a molecular level, opening up new opportunities for drug discovery and design.

\subsection{Fully automated drug discovery}
Full automation of the drug discovery process is an ambitious goal that many researchers are working towards. The aim is to develop systems that can perform multiple iterations of the design-make-test process autonomously, with all decision-making handled by the system.

Recent developments in antibody design demonstrate the potential of this approach. Researchers have used machine learning to design and test thousands of potential antibody candidates, identifying promising new drug targets with unprecedented speed and accuracy.

However, achieving full automation of drug discovery will require integrating all of the models and workflows mentioned above, and developing new approaches to optimize decision-making and resource allocation. Reinforcement learning, in particular, could be the ultimate embodiment of this vision, enabling systems to learn from experience and continually improve their performance.

That said, the computational resources required for such an approach may be prohibitive in the near term, and significant technological breakthroughs will be required to make full automation of drug discovery a practical reality. Nonetheless, the potential benefits of such an approach are enormous, with the potential to transform the field of drug discovery and accelerate the pace of new drug development.
