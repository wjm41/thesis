\chapter{Outlook} \label{ch:outlook}

The research presented in this thesis explore ways in which data-driven approaches based on machine learning can be leveraged in the design-make-test cycle of drug discovery. In each of the three steps of `design', `make', and `test' we encounter the same underlying challenge, of grappling with the practical difficulties of a drug discovery campaign where we must make full use of the limited data available.

In Chapter \ref{ch:fresco} we looked at how to leverage fragment-protein structures from a crystallographic fragment screen to hit discovery in the absence of any bioactivity data. Using an unsupervised learning approach, we learn the geometric distribution of pharmacophores from the fragment-protein complexes, and use these to screen potential molecules for bioactivity. We showed that this approach outperforms docking on distinguishing active compounds from inactive ones on retrospective data. Further, we prospectively found novel hits for SARS-CoV-2 Mpro and the Mac1 domain of SARS-CoV-2 non-structural protein 3 by virtually screening a library of 1B molecules.

Chapter \ref{ch:ranking} takes us to the early stages of hit-to-lead molecular optimisation where bioactivity data is limited, noisy, and dominated by inactive molecules. We overcame this challenge with a learning-to-rank framework via an ML model that predicts whether a compound is more or less active than another. This approach allowed us to make use of inactive data and threshold the bioactivity differences above measurement noise, and validation on retrospective data for SARS-CoV-2 Mpro showed that we can outperform docking on ranking ligands. Combining this model with AI-based synthesis tools, we prospectively screened a library of 8.8M molecules to arrive at a potent compound with a novel scaffold.

While AI-based synthesis tools have already shown demonstrable success in accelerating the synthesis of new molecules, they are still prone to failure and suffer from a lack of transparency in their decision making due to their black-box nature. To address this, in Chapter \ref{ch:transformer} we showcased a workflow for quantitatively interpreting a state-of-the-art deep learning model for reaction prediction. By analysing chemically selective reactions, we showed examples of correct reasoning by the model, explain counterintuitive predictions, and identify Clever Hans predictions where the correct answer is reached for the wrong reason due to dataset bias.

In Chapter \ref{ch:testing} we explored how to accelerate testing procedures by applying machine learning on bioactivity data from nanomolar-scale high-thoughput chemistry. While this experimental technique greatly increases the number of molecules that can be tested, there is additional noise resulting from having to assay crude reaction mixtures instead of pure samples. Nevertheless, we showed that machine learning models trained on this data is able to cut through this noise and identify a false negative assay measurement, as well as prospectively screen a library of ~62K molecules to discover new SARS-CoV-2 Mpro inhibitors just as potent as those from the original assay.

\section{Directions for Future Research}

While we have explored some methods to accelerate the design-make-test cycle in this thesis, there is still much more work to be done to bring us closer to the dream of efficient and automated drug discovery. Below, we outline some of the most prominent directions for future research.

\subsection{Deploying and Extending ML-based Synthesis Tools}
Machine learning-based models for synthesis prediction have made significant strides in recent years, and are already sufficientily accurate to be deployed in industry for synthesis route design \cite{Tu2023deployment}. However, translating these models into a practical tool that can be used by non-specialists is non-trivial. Connecting these models with commercial compound databases, updating models in response to changes in stock availability, and developing more user-friendly interfaces are fruitful engineering challenges for enabling more widespread adoption of these tools.

An intriguing extension of this idea is allowing a large language model (LLM) to access ML-based synthesis tools. Large language models, using the same architectures on which many ML synthesis tools are based on, have demonstrated incredible success in natural language processing as well as in understanding programming languages \cite{OpenAI2021GPT3,Chowdhery2022Palm, Touvron2023Llama}. An exciting use-case is to allow LLMs to make API calls to other software, meaning that one can abstract away complex programming tasks by just giving a natural language description of the task to the LLM. A recent example demonstrated how GPT-4, a powerful LLM, can perform molecular queries in the PubChem database, determine whether the molecule is purchasable, find a supplier that sells it and either purchase the compound or draft an email to a synthesis CRO to order \cite{OpenAI2023GPT4}. Integrating LLMs with ML-based synthesis tools, or augmenting LLMs by training them to understand SMILES and/or SMARTS, could extend such capabilities and facilitate complex user requests in synthesis planning.

In tandem with optimisng model deployment, continued improvement in the accuracy of synthesis predictions can be acheived in a number of ways. Novel model architectures that better reflect the underlying structure of chemical reaction data, both the graph nature of molecules as well as the correlations between chemical reactions from the same patent, could potentially improve the accuracy of synthesis predictions. The incorporation of proprietary reaction data from industry should also lead to improved performance, although there are difficulties in standardising data from electronic laboratory notebooks (ELNs) \cite{Jablonka2022OpenChemistry} and there has been conflicting evidence regarding its usefulness \cite{Thakkar2020DatasetsSynthesis, Wiest2023DatasetYields}. Finally, the creation of better benchmarks, likely incorporating disclosed industrial data \cite{Kearnes2021OpenReaction}, can help ensure that models are evaluated under realistic conditions, providing a more accurate and robust representation of their capabilities.

Last but not least, there is much room for improvment in the prediction of optimal reaction conditions, catalysts, and reaction yields, \cite{Ahneman2018PredictingCoupling, Schwaller2021YieldPrediction, Probst2022ReactionClassification} which are not currently handled by most ML-synthesis tools.  While existing models are effective at predicting the outcomes of small-scale chemical reactions, they may not be as accurate when it comes to predicting the behavior of reactions that must be scaled up for large-scale production. Including the modelling of reaction yields and reaction conditions, for example via training on high-throughput experimentation data \cite{king2022probing, xu2022roadmap} combined with bayesian optimisation \cite{Shields2021BayesOptReaction}, would allow for greater precision overall, and expand the scope of these models beyond small-scale reactions to bridge the gap between research and production.

\subsection{Addressing Data Scarcity}
A common challenge in drug discovery is the limited availability of data. While it is true that data scarcity is a persistent issue, it is also true that we are not making the most of the data that we do have \cite{Bender2021part2}. Existing datasets can be enhanced by utilizing them in more effective ways, and by designing models that take into account the unique structure inherent in the data. For example, imputation-based methods explicitly consider data sparsity, filling in missing data points with estimates derived from other data points which allows researchers to make use of incomplete datasets and exploit correlations between different endpoint measurements \cite{Irwin2020Imputation, Irwin2021Imputation, Tse2021OpenMalaria}. Transfer learning approaches involve using knowledge gained from one dataset to inform the learning process in another domain, and there is a large scope for designing models that can utilise abundant noisy data to improve the prediction of low-data properties \cite{Wenzel2019ADMETox, Obrezanova2022InVivo}. By developing and utilizing models that are specifically designed for low-data regimes instead of merely adapting models performant in data-rich scenarios, we can make more accurate predictions and gain insights that would not be possible with traditional models.

Federated learning is another promising approach that can help overcome data scarcity by building models that can learn from proprietary data while maintaining data privacy \cite{Li2020FedLearning}. The pharmaceutical industry posesses orders of magnitude more data on experimental measurements than publicly available but that data is often proprietary and cannot be shared. Federated learning approaches allow us to pool data from multiple industrial partners while maintaining privacy and security of the underlying data, allowing us to build models more accurate than possible by any individual partner. A landmark example of this is the MELLODDY project \cite{Wouter2022Melloddy, Oldenhof2022Melloddy}, which brought together ten pharmaceutical companies and aggregated over 2.6+ billion confidential experimental data points for 21+ million different small molecules and 40+ thousand on-target and pharmacokinetics assays. The massive increase in collective training data, boosted predictive performance particularly for pharmacokinetics and safety panel assays. The methodology of this project is a strong foundation on which to build more powerful federated learning platforms in the future.

In parallel to maximising the use of existing data, there are also exciting opportunities to use machine learning methods to leverage noisy but high-throughput experimental techniques in drug discovery. One example of such a technique is DNA-encoded libraries, which enable the synthesis and screening of large numbers of compounds in a relatively short amount of time. While the resulting data may be noisy, the sheer volume of data generated makes it a valuable resource for building ML models \cite{McCloskey2020DNALibrary, Blay20221DELTox, Lim2022DELCountML}. There is a large scope for developing models that can leverage data from other under-utilised high-throughput techniques, from microfluidics platforms \cite{Dittrich2006microfluidics, Skardal2016chip} to high-throughput crystallography \cite{Blundell2002HighThroughputCrystallography, Schiebel2016HighThroughput}, to deliver novel insights in drug discovery.

\subsection{Integration of Protein Bioinformatics}
Although bioactivity involves the complementary interactions between a small molecule and its protein target, the focus of machine learning methods in drug discovery has largely been on molecules alone while the protein component has been largely neglected. Recent stunning advances in machine learning for protein bioinformatics, such as AlphaFold \cite{Jumper2021AlphaFold} and RFDiffusion \cite{Watson2022RfDiffusion}, suggest that ML bioinformatics models have a rich understanding of protein structure and function. It is clear that by integrating these models and workflows with those for ML in chemistry and cheminformatics, we may be able to make significant breakthroughs in bioactivity prediction which remains a fundemental challenge in drug discovery.

Expanding beyond bioactivity prediction, these models and workflows should improve the modelling of small molecule interactions with proteins in general. This should also have potential for improving the prediction of pharmacokinetics and toxicity properties, areas in which data scarcity and assay variability pose an even greater challenge than bioactivity prediction \cite{Bhhatarai2019ADMETox, Wenzel2019ADMETox,Goller2020BayerADMET, Obrezanova2022InVivo}. Greater accuracy in predicting in-vivo measurements from in-vitro data would allow us to design safer drugs and reduce clinical-stage attrition in the latter stages of drug development.

Demonstrable acheivements in predicting protein function \cite{Bileschi2022ProteinAnnotation} as well as enzyme design \cite{Yeh2023DeNovoLuciferase} also open the door for designing efficient biocatalysed synthesis routes for otherwise unsynthesisable drug candidates. This could have a particular impact in latter stages of drug design where functionalisation of a complex drug candidate is required to optimise its pharmacokinetics while retaining bioactivity. The inclusion of basic enzyme information into ML-based synthesis tools has already been shown predictive capabilities on retrospective data \cite{Probst2022Biocatalysis}, and it is likely that successful prospective validation experiments will follow.

Extrapolating further, the ability to predict molecular interactions with a diverse range of proteins could also be used to determine molecular mechanisms of action, as well as identifying new drug targets \cite{Schenone2013TargetID}. By combining insights from both chemical and biological data, we can gain a more comprehensive understanding of how small molecules interact with the body at a molecular level, opening up new opportunities for drug discovery and design.

\subsection{Full Automation of Drug Discovery}
Full automation of the drug discovery process is an ambitious goal that is the ultimate destination of research in computational drug design. Developing systems that can perform multiple iterations of the design-make-test process autonomously, with no human intervention in the decision-making process, is a long-term goal that is still far from being realised \cite{Coley2019AutonomousProgress,Coley2019AutonomousOutlook, Schneider2018AutomatingDrugDiscovery}.

At the moment the most advanced systems still require significant human intervention in the decision-making process, such as constraining the search space of potential drug designs, ordering/performing the synthesis/assaying of selected candidates, and interpreting experimental results \cite{Goldman2022ChemicalDesignLevels}. Achieving full automation of drug discovery will require integrating all of the models and workflows mentioned above, and developing new approaches to optimize decision-making and resource allocation. Incorprating generative models with reinforcement learning \cite{Popova2018DeepRL, Zhou2019Optimization, born2019paccmannrl,Chenthamarakshan2022IBMGenCoV}, in particular, could be the ultimate embodiment of this vision, enabling systems to learn from experience and continually improve their performance.

That said, the computational and material resources required for training such an approach may be prohibitive in the near/medium term, and significant technological breakthroughs will be required to make full automation of drug discovery a practical reality. Nonetheless, the potential benefits of such an approach are enormous, with the potential to transform the field of drug discovery and accelerate the pace of new drug development.
