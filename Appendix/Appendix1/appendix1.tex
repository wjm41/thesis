
\chapter{Computational Details} \label{appendix:details}

\section{Docking against SARS-CoV-2 Mpro} \label{appendix:docking}
All molecules synthesised by the COVID Moonshot Consortium were docked against structure x2908 reported by Diamond XChem \cite{Douangamath2020XChem}. We use the “Classic OEDocking” floe v0.7.2 as implemented in the Orion 2020.3.1 Academic Stack (OpenEye Scientific). Omega was used to enumerate conformations (and expand stereochemistry) with up to 500 conformations. FRED was used for docking in HYBRID mode using the x2908 bound ligand. The docked poses of the ligands were scored using the Chemgauss4 scoring function.

\section{FRESCO} \label{appendix:fresco}

The workflow for implementing efficient and accurate KDE fitting utilises several software packages. The Improved Sheather-Jones algorithm for KDE selection utilises the implementation in \href{https://kdepy.readthedocs.io/en/latest/index.html}{\texttt{KDEpy}}. Each KDE is then constructed using the chosen bandwidths with \href{https://scikit-learn.org/stable/}{\texttt{scikit-learn}} \cite{scikit-learn} for technical ease of use in evaluating probabilities. The \texttt{scikit-learn} implementation relies on a relatively slow tree-based algorithm that searches over the training datapoints - to increase the computational efficiency of inference for virtual screening, computationally fast approximations of the KDEs are made using the \texttt{scipy} \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html#scipy.interpolate.interp1d}{\texttt{interp1d}} function \cite{scipy}.

The pharmacophore processing, substructure filtering, and butina clustering workflow was implemented using the \href{https://www.rdkit.org/docs/index.html}{\texttt{rdkit}} \cite{rdkit} Python package.

All data and code used for this work can be found in the GitHub repo \url{https://github.com/wjm41/frag-pcore-screen}.

\section{Ranking model} \label{appendix:ranking}

The MLP model for ranking compounds was implemented using the FastAI Tabular framework \cite{howard2018fastai}. We found that the default hyperparameters often resulted in overfitting, so we use a smaller network with non-negligible dropout for additional regularization. Two layers of size 10 and 5 were used, with a dropout of 0.3 applied to both layers. The network was trained for 5 epochs with the Adam optimizer \cite{Kingma2014Adam} and the 1cycle cyclical learning rate scheduler \cite{smith2018superconvergence}.

The reaction routes were generated using \href{https://app.postera.ai}{Manifold} \cite{PosteraManifold}, an ML-based synthesis tool and search engine.

The model training sets, screening library, and code implementation are available at \url{https://github.com/wjm41/mpro-rank-gen}.

\section{Molecular Transformer interpretation} \label{appendix:transformer}
The Molecular Transformer architecture used is based on the model described in Schwaller et. al. \cite{Schwaller2019MolecularPrediction}.

The model uses a 256 dimensional learnt embedding for each SMILES token. The encoder and the decoder are both made up of 4 standard transformer layers and dropout is applied with probability 0.1 \cite{Srivastava2014dropout}. For weight optimization the Adam optimizer is used and the model is trained for 500 000 steps. Checkpoints are saved every 10 000 steps and the final model is obtained by averaging the weights of the last 20 checkpoints for USPTO.

The model was implemented with OpenNMT-py package \cite{Klein2017} which makes use of the PyTorch framework \cite{paszke2019pytorch}.

All code used for implementing the attribution tools for the Molecular Transformer, generating the artificial Friedel-Crafts dataset, and Tanimoto-splitting USPTO can be accessed at \url{https://github.com/davkovacs/MTExplainer}. The USPTO dataset used to train the model \cite{Lowe2012, Jin2017}, as well as the Tanimoto similarity-based train/test splits of USPTO can also be found in the GitHub repo.

\section{Crude bioactivity modelling} \label{appendix:crude}
The RF model was implemented with default hyperparameters in scikit-learn \cite{scikit-learn}. The code for implementing the RF model, as well as the bioactivity data, screening library and chosen molecules are available on \url{https://gitlab.com/wjm41/noisyamides}.

The GP model was also implemented in scikit-learn \cite{scikit-learn} with default length scale hyperparameters of 1.0 for the radial basis function as well as Matern kernels. The code for implementing the GP model can be found at \url{https://github.com/emmaking-smith/Moonshot}.